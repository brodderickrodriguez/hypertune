{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "The `hypertune` package is designed for easy hyperparameter tuning of Machine Learning algorithms. It allows for a user to specify an algorithm's parameters as `hypertune` `Parameter`s. `hypertune` has six types of `Parameter`s: `ConstantParameter`, `ContinuousParameter`, `DiscreteParameter`, `TupleParameter`, `CategoricalParameter`, and `ObjectParameter`. Each of these specify unique behavior and types. For a more detailed explaination, please see the notebook on Understanding `hypertune` `Parameter`s.\n",
    "\n",
    "The purpose of this notebook is to demonstrate how one can use `hypertune` in their existing implementations. `hypertune`'s `HyperTune` object is responsible for tuning your algorithm's hyperparameters by passing it: \n",
    "* `algorithm` - the non-initialized algorithm, \n",
    "* `parameters` - a list of `Parameter`s,\n",
    "* `train_func`, `objective_func` - the training and objective functions or methods, \n",
    "* `train_func_args=None` - the arguments that will be passed to `train_func`. In most use cases this will be set to `(X_train, y_train)`,\n",
    "* `objective_func_args=None` - the arguments that will be passed to `objective_func`. In most use cases this will be set to `(X_test, y_test)`,\n",
    "* `max_evals=100` - the number of iterations,\n",
    "* `optimizer=optimizers.PSO()` - the `HyperTune.Optimizer`,\n",
    "* `maximize=True` - if the problem is maximization or minimization, and\n",
    "* `num_replications=1` - the number of replications.\n",
    "\n",
    "As an example, lets consider this hypothetical Machine Learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypertune as ht\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AI:\n",
    "    def __init__(self, eta, max_iter=100):\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def accuracy(self, *args, **kwargs):\n",
    "        return np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, AI is the target machine learning algorithm which has the parameters `eta` and `max_iter`. The methods `train()` and `accuracy()` are the desired training and objective functions, respectivley. `accuracy()` returns a random number as `AI` is a fabricated example.\n",
    "\n",
    "Defining `AI`'s hyperparameters is simply done by creating surrogate definitions for them that `hypertune` can decode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = ht.ContinuousParameter(name='eta', lower_bound=1e-5, upper_bound=1e-1)\n",
    "max_iter = ht.DiscreteParameter(name='max_iter', lower_bound=10, upper_bound=1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined `eta` as a `ContinuousParameter` so `HyperTune` can decode it to a continuous value. Similarly, we define `max_iter` as a `DiscreteParameter` so it is decoded to a discrete value. Tuning `eta` and `max_iter` is done simply by defining a `HyperTune` object and calling its `tune()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613] 0.9406661298000178\n",
      "[0.94066613 0.94066613 0.94066613 0.94066613 0.94066613 0.94066613\n",
      " 0.94066613 0.94066613 0.94066613 0.94066613]\n",
      "{'objective_fn_value': 0.9406661298000178, 'params': {'eta': 0.009557746586191261, 'max_iter': 253}}\n"
     ]
    }
   ],
   "source": [
    "hypers = [eta, max_iter]\n",
    "\n",
    "tuner = ht.HyperTune(algorithm=AI, parameters=hypers, train_func=AI.train, objective_func=AI.accuracy)\n",
    "\n",
    "results = tuner.tune()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we were able to maximize our `accuracy()` function to `0.999` using `eta = 0.0634` and `max_iter = 821`.\n",
    "Additionally as a note, defining hypers as `[eta, max_iter]` is equivilant to defining it as `[max_iter, eta]`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using `AI`'s class methods for training and determning performance, `HyperTune` can take functions external to the class as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603] 0.5288060316654928\n",
      "[0.52880603 0.52880603 0.52880603 0.52880603 0.52880603 0.52880603\n",
      " 0.52880603 0.52880603 0.52880603 0.52880603]\n",
      "{'objective_fn_value': 0.5288060316654928, 'params': {'eta': 0.09406720631870379, 'max_iter': 920}}\n"
     ]
    }
   ],
   "source": [
    "def mse(algo, *args, **kwargs):\n",
    "    return np.random.rand()\n",
    "\n",
    "tuner = ht.HyperTune(algorithm=AI, parameters=hypers, train_func=AI.train, objective_func=mse, maximize=False)\n",
    "\n",
    "results = tuner.tune()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we were able to minimize `mse()` to `0.002` using `eta = 0.009` and `max_iter = 413` by setting `maximize = False`.\n",
    "\n",
    "Furthermore, `HyperTune` can handle algorithms with default parameters. Meaning, as `AI.max_iter` has a default value of `100` you need not to specify it as a `Parameter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601] 0.2500600977926756\n",
      "[0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601 0.2500601\n",
      " 0.2500601 0.2500601 0.2500601]\n",
      "{'objective_fn_value': 0.2500600977926756, 'params': {'eta': 0.05288531510623263}}\n"
     ]
    }
   ],
   "source": [
    "hypers = [eta]\n",
    "tuner = ht.HyperTune(AI, hypers, AI.train, AI.accuracy)\n",
    "\n",
    "results = tuner.tune()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we were able to maximize `accuracy()` to `0.988` by setting `eta = 0.061`. \n",
    "\n",
    "For an explicit example, lets take a look at Scikit-learn's `MLPClassifier`. We first need to define a dummy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3) (100, 3) (400,) (100,)\n"
     ]
    }
   ],
   "source": [
    "# create 500 data instances each with 3 features and a discrete label either `0` or `1`.\n",
    "X = np.random.rand(500, 3)\n",
    "y = np.random.rand(500)\n",
    "\n",
    "y[y > 0.5], y[y <= 0.5] = 1, 0\n",
    "\n",
    "# (+/-) to make it more linearly seperable\n",
    "X[y > 0.5] += 0.25\n",
    "X[y <= 0.5] -= 0.25\n",
    "\n",
    "X_train, X_test = X[:400], X[400:]\n",
    "y_train, y_test = y[:400], y[400:]\n",
    "train = X_train, y_train\n",
    "test = X_test, y_test\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.935] 0.935\n",
      "[0.93] 0.93\n",
      "[0.9275 0.93   0.935  0.9275 0.9275 0.93   0.9275 0.93   0.9275 0.935 ]\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.93   0.93   0.935  0.93   0.93   0.9275 0.93   0.93   0.93   0.93  ]\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.9325] 0.9325\n",
      "[0.93   0.9275 0.935  0.93   0.93   0.93   0.9325 0.93   0.93   0.9275]\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.9325] 0.9325\n",
      "[0.935] 0.935\n",
      "[0.93   0.9275 0.935  0.93   0.93   0.93   0.935  0.9325 0.93   0.9275]\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.93] 0.93\n",
      "[0.925] 0.925\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.93   0.93   0.935  0.9275 0.9275 0.9275 0.93   0.93   0.9275 0.925 ]\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.93   0.9275 0.935  0.93   0.93   0.9275 0.93   0.93   0.93   0.9275]\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.935] 0.935\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.93   0.9275 0.935  0.93   0.93   0.9275 0.93   0.93   0.93   0.9275]\n",
      "[0.9275] 0.9275\n",
      "[0.925] 0.925\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9325] 0.9325\n",
      "[0.9275 0.925  0.935  0.9275 0.9275 0.9275 0.9275 0.9275 0.9275 0.9325]\n",
      "[0.93] 0.93\n",
      "[0.925] 0.925\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9325] 0.9325\n",
      "[0.935] 0.935\n",
      "[0.93   0.925  0.935  0.9275 0.93   0.9325 0.9275 0.9275 0.9275 0.935 ]\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.9325] 0.9325\n",
      "[0.9275 0.93   0.935  0.9275 0.9275 0.935  0.9275 0.9275 0.9275 0.9325]\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.93] 0.93\n",
      "[0.935] 0.935\n",
      "[0.9275] 0.9275\n",
      "[0.9275] 0.9275\n",
      "[0.935] 0.935\n",
      "[0.9325] 0.9325\n",
      "[0.9275 0.935  0.935  0.935  0.93   0.935  0.9275 0.9275 0.9275 0.9325]\n",
      "{'objective_fn_value': 0.935, 'params': {'learning_rate_init': 0.019187013673161956, 'max_iter': 9278}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "learning_rate_init = ht.ContinuousParameter('learning_rate_init', lower_bound=1e-5, upper_bound=0.1)\n",
    "max_iter = ht.DiscreteParameter('max_iter', lower_bound=5e2, upper_bound=10e4)\n",
    "\n",
    "hypers = [learning_rate_init, max_iter]\n",
    "tuner = ht.HyperTune(MLPClassifier, hypers, MLPClassifier.fit, MLPClassifier.score, test, train)\n",
    "\n",
    "results = tuner.tune()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we were able to maximize `MLPClassifier.fit` to `0.98` by setting `learning_rate_init = 0.0316` and `'max_iter = 47116`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
